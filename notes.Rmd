---
title: "Notes on Applied Econometrics"
author: "Pranav Singh"
date: "Spring 2017"
output:
  html_document: default
  pdf_document: default
ouptut:
  pdf_document:
    toc: yes
    toc_depth: 2
---
```{r setup}
library(knitr)
# opts_knit$set(root.dir='~/Google Drive/Dalton State College/2017 Spring/ECON-4101-Applied-Econometrics/')
opts_chunk$set(cache = T)
```

# Discrete Random Variables
Let X = outcome of flipping a fair coin.
  X = 0 if heads
  X = 1 if tails
p_1 = P(X = 0) = 1/2
P_2 = P(X = 1) = 1/2

- `p_1` and `p_2` define Probability (mass/density) function

```{r}
# evansresearch.us/DSC/Spring2017/ECMT/
home.prices <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/hprice.csv', header = T)
```

# Linear regression
Economic Model  
Consumer Expenditures = f(DisposableDisposable Income)  
$y = \beta_0 + \beta_1x$  
$E[y | x] = E[\beta_0 + \beta_1x]$  
$\sigma^2 \equiv VAR(y)$  
$COV(y_i, y_j) = \emptyset \quad \forall i \neq j$  
$x_i \neq x_j$ (i.e. $x$ is not a random variable)  
$y \sim N(\beta_0 + \beta_1x, \sigma^2)$  
  
Assumption 1: $y = \underbrace{\beta_0 + \beta_1x}_{systeatic} \quad + \underbrace{e}_{unobserved}$  
Assumption 2: $E[e] = 0$  
Assumption 3: $VAR(e) = \sigma^2$  
Assumption 4: $COV(e_i, e_j) = 0$  
Assumption 5: $x_i \neq x_j$ for at least 2 values  
Assumption 6: $e \sim \mathcal{N}(0, \sigma^2)$  

We want to minimize the least squares error $LSE = (\hat{y} - \hat{r}(x))^2$, where $\hat{r}(x) = \hat{\beta_0} + \hat{\beta_1}x$. Then the estimated error is $\hat{e} = \hat{y} - \hat{r}(x)$.


```{r}
# https://stackoverflow.com/questions/7549694/ggplot2-adding-regression-line-equation-and-r2-on-graph
lm_eqn = function(m) {

  l <- list(a = format(coef(m)[1], digits = 2),
      b = format(abs(coef(m)[2]), digits = 2),
      r2 = format(summary(m)$r.squared, digits = 3));

  if (coef(m)[2] >= 0)  {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  } else {
    eq <- substitute(italic(y) == a - b %.% italic(x)*","~~italic(r)^2~"="~r2,l)    
  }

  as.character(as.expression(eq));                 
}
```

```{r}
library(ggplot2)
dispIncome.consump <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm04YC.csv', header=T)
y <- dispIncome.consump$consump; names(y) <- 'Consumption'
x <- dispIncome.consump$dispinc; names(x) <- 'Disposable Income'
boxplot(cbind(x,y), horizontal=T, col='gray')
lm.res <- lm(y ~ x)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = y ~ x) +
  annotate("text", x = min(x) + .2*(max(x) - min(x)), y = max(y), label = lm_eqn(lm.res), colour="black", size = 5, parse=TRUE)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + geom_point() + geom_smooth(method = 'loess')

cov(x,y)
cor(x,y)
res1 <- cor.test(x,y); res1
if(res1$p.value < 0.05) {
  message('significantly different from 0 at alpha=.05')
} else {
  message('not significantly different from 0 at alpha=.05')
}

# Manual calculation of estimated linear regression parameters
n <- length(x)
b1.num <- n*sum(x*y) - sum(x)*sum(y)
b1.denom <- n*sum(x^2) - sum(x)^2
b1 <- b1.num / b1.denom
b0 <- mean(y) - b1*mean(x)
round(b1,6) == round(cor(x,y)*sd(y)/sd(x), 6) # verify b1 = r * s_y / s_x
print(c(b0, b1))
print(lm.res$coefficients)
plot(x,y)
abline(b0, b1, lty=4, col='blue')
abline(h=mean(y), v=mean(x))
```

# Analysis of Variance

Total Sum of Squares = $TSS = \sum(y - \bar{y})^2$  
Sum of Squared Errors = $SSE = \sum(y - \hat{y})^2$  
Sum of Squares Regression = $SSR = \sum(\hat{y} - \bar{y})^2$  
$TSS = SSE + SSR$

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | TSS | n - 1 |

$SEE = \text{standard error of estimate} = \sqrt{MSE}$  

## Goodness of Fit Tests
$R^2 = \text{coefficient of determination} = \frac{SSR}{TSS}$  
$R^2_{Adj} = 1 - (1 + R^2)\theta$, where $\theta = \frac{n-1}{n-k-1}$ where $k$ is the number of independent variables  

### F Statistic
$H_0 : \beta_1 = 0$
$H_1 : \beta_1 \neq 0$
$\alpha = .05$
$F_{df_1, df_2} = \frac{MSR}{MSE} \text{ where } df_1 = 1 ,\; df_2 = n-2$
Reject $H_0$ if $F_{1,n-2} > F_{crit}$  
```{r}
data(mtcars)
y <- mtcars$mpg
x <- mtcars$hp
plot(x, y, pch=1, col='blue', ylab = 'mpg', xlab = 'hp')
ybar <- mean(y)
abline(h=ybar, col='blue', lwd=.5)
n <- length(y)
mod1 <- lm(y ~ x)
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
abline(coef = c(b0,b1), lty=4, col='orange')
yhat <- b0 + b1*x
points(x,yhat,pch=24,col='red')
ssr <- sum((yhat - ybar)^2)
df <- 1
msr <- ssr / df
sse <- sum( (y - yhat)^2 )
tss <- ssr + sse
r2 <- ssr / tss
round(r2, 4) == round(summary(mod1)$r.squared, 4)
k <- 1
r2adj <- 1 - (1-r2)*(n-1)/(n-k-1)
round(r2adj, 4) == round(summary(mod1)$adj.r.squared, 4)
round(tss / (n-k), 4) == round(var(y), 4)
mse <- sse / (n - k - 1)
see <- sqrt(mse)
fstat <- msr / mse
round(fstat, 4) == round(summary(mod1)$fstatistic['value'], 4)

fcrit <- qf(0.95, k, n - 2)
pf(fstat, k, n-2, lower=F)
```

# CM07
```{r}
data <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/tabb777.csv', header=T)
```
```{r}
str(data)
data$fuel <- data$fuel/1000
plot(data$hours, data$fuel)
```
```{r}
lm.mod <- lm(data = data, fuel ~ hours)
summary(lm.mod)
anova(lm.mod)
confint(lm.mod)
```
```{r}
newdata <- data.frame(hours=5:10)
cbind(newdata, predict(lm.mod, newdata, interval='confidence'))
cbind(newdata, predict(lm.mod, newdata, interval='prediction'))
```

```{r}
# download.file(url = 'http://evansresearch.us/DSC/Spring2017/ECMT/Code/ecmtUtil.R', destfile = './code/ecmUtil.R')
source('./code/ecmUtil.R')
addCIPI2Plot(data$hours, data$fuel)
```

# CMO8
### Dummy variables
Dummy variables represent qualitative (categorical) characteristics. We define
\[ 
D = 
\begin{cases}
  1 &\text{ if the characteristic is observed} \\
  0 &\text{ otherwise}
\end{cases}
\]
Dummy variable trap (including separate features for both D and ~D in the model) can lead to perfect collinearity problem.

### Functional Form
\[ \text{linear: }\; \hat{y} = a + bx + \epsilon \]
\[ \text{log-linear: }\; \ln{y} = a + bx + \epsilon \]
\[ \text{linear-log: }\; y = a + b\ln{x} + \epsilon \]
\[ \text{log-log: }\; \ln{y} = a + b\ln{x} + \epsilon \]
\[ \epsilon = \frac{ \% \Delta{Q_d} }{ \% \Delta{P} } \approx \underbrace{ \frac{ \frac{\Delta{Q_d}}{\overline{Q}}}{\frac{\Delta{P}}{\overline{P}}} }_{\text{midpoint method}} \]

```{r}
mpg <- mtcars$mpg
am <- mtcars$am # Dummy variable: am = 0 if auto, 1 if manual transmission
cyl <- mtcars$cyl
mod1 <- lm(mpg ~ am)
summary(mod1)
```
```{r}
dummy1 <- as.integer((am == 1) & (cyl == 8))
mod2 <- lm(mpg ~ dummy1)
summary(mod2)
plot(dummy1, mpg)
abline(coef(mod2))
```

### Consumption ~ Disposable.Income
\[ Real = \frac{Nominal}{CPI} * 100 \]
```{r}
dfm <- read.csv("http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm08YC.csv")
str(dfm)
y <- 100 * dfm$consump / dfm$cpi
x <- 100 * dfm$dispinc / dfm$cpi
summary(lm(y ~ x))
ly <- log(y)
lx <- log(x)
summary(lm(ly ~ lx))
```

# Multiple Regression
## Gauss-Markov Assumptions
1. Linear in parameters 
$$
\begin{aligned}
y &= \beta_0 + \sum{B_ix_i} + \epsilon \\
\ln(y) &= \beta_0 + \sum{B_ix_i} + \sum{B_j\ln(x_j)} + \epsilon \\
\end{aligned}
$$
But we can **NOT** have $y = \beta_0\cdot\beta_1 + \beta_1^2 \cdot x_1$ or the like.  
2. Random Sample $(x_i, y_i)$
3. Zero Conditional Mean \[ E(\epsilon_i|x_i) = 0 \]
4. No perfect collinearity (between any of the regressors)
    - example that fails this test: 
      \[ \text{height}_{women} = \beta_0 + \beta_1 \cdot \text{age}_{years} + \beta_2 \cdot \text{age}_{months} + \epsilon \]
5. Constant Variance \[ \mathrm{VAR}(\epsilon \vert x_i) = \sigma^2 \]
6. No Serial Correlation ("Auto-Correlation") \[ \mathrm{COV}(\epsilon_i, \epsilon_j) = 0 \]

Assumptions 5 and 6 are referred together as the **Spherical Assumptions**.

## Global F-Test
$$
\begin{aligned}
H_0 &: \beta_1 = \beta_2 = \ldots = \beta_n = 0 \\
H_1 &: \neg H_0 \\
\text{test statistic (TS): } F &= \frac{MSR}{MSE} \\
\alpha &= \mathrm{signif}(5\%) \\
\mathrm{Reject} H_0 \text{ if } F &> F_{crit, \alpha} \\
F_{crit}: & \text{ numerator df = K = # of independent vars} \\
& \text{ denominator df = N - (K + 1) = # of observations} \\
\end{aligned}
$$
### Analysis of Variance

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | SST | n - 1 |

\[ R^2 = \frac{SSR}{SST} \]
\[ S_{y|x} = \sqrt{MSE} \]

### Multi-collinearity
#### Symptoms  
* independent variables not significant yet they should be good predictors
* parameter estimates with reversed signs than what we expected
* adding/removing variables leads to substantial changes in other parameter estimates  

#### Determination
* correlation coefficient between predictors has large magnitude: $|r| > 0.7$
* **Variance Inflation Factor (VIF)**: $\mathrm{VIF} = \frac{1}{1 - R_j^2}$ where $j$ is each random variable
\[ y = \beta_0 + \sum{\beta_ix_i} \]
\[ x_1 = \alpha_0 + \sum_{i \neq 1}{\alpha_ix_i} \]
\[ R_1^2 \implies \mathrm{VIF}_1 = \frac{1}{1-R_1^2} \]
\[ R_2^2 \implies \mathrm{VIF}_2 = \frac{1}{1-R_2^2} \]

**Iterative Elimination of Offensive Variables**
1. For each independent variable, calculate `VIF`.
2. Ask the question: Are all `VIF`s < 10? If yes, then quit. Otherwise, continue.
3. Remove $\underset{x_i}{\mathrm{argmax}}(VIF_{i})$
4. Return to Step 1.

```{r}
df <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/sp500.csv')
# SP500: S&P500 index
# TB3MS: 3-month treasury bill (%)              (Hypoth: - parameter)
# NAPMNOI: new manufacturing orders (index)     (Hypoth: + parameter)
# INDPRO: industrial production (index)         (Hypoth: + parameter)
# M1: money supply                              (Hypoth: + parameter)
# HSTART: new house construction (in thousands) (Hypoth: + parameter)
# WEARN: average weekly earnings                (Hypoth: - parameter)
# PPI: producer price index (index)             (Hypoth: - parameter)
str(df)
dates <- df$YYYY.MM.DD
df$YYYY.MM.DD <- NULL
```
```{r}
mod1 <- lm(SP500 ~ ., df)
summary(mod1)
```
```{r, fig.height=6, fig.width=6}
require(corrplot)
cor1 <- cor(df)
kable(cor1)
corrplot(cor1)
```
```{r}
require(car)
crPlots(mod1)
hccm(mod1)
```
```{r}
df.x <- df[,setdiff(colnames(df), 'SP500')]
mod2 <- lm(TB3MS ~ ., df.x)
r2 <- summary(mod2)$r.squared
vif_tb <- 1 / (1-r2)
vif_tb
```
```{r}
# Using car package's vif method
require(car)
xy <- df
vif <- vif(mod1); vif
while (sort(vif, decreasing = T)[1] >= 10) {
  removeMe <- names(sort(vif, decreasing = T)[1])
  xy <- xy[,setdiff(colnames(xy), removeMe)]
  mod2 <- lm(SP500 ~ ., xy)
  vif <- vif(mod2); vif
}
vif
summary(mod2)
```
```{r}
require(lubridate)
dates <- ymd(dates)
training_idx <- which(dates < '2015-01-01')
train.xy <- xy[training_idx,]
test.xy <- xy[-training_idx,]
mod3 <- lm(SP500 ~ ., train.xy)
summary(mod3)
# Estimate of PPI changes signs and becomes statistically insignificant 
# When we built model on limited training data. So just drop it from model for now
mod3 <- lm(SP500 ~ TB3MS + NAPMNOI + INDPRO + HSTART, train.xy)
preds <- predict(mod3, test.xy)
test.results <- cbind(preds, test.xy)
test.rmse <- sqrt(mean((test.results$preds - test.results$SP500)^2))
```

# Model Specification
**Importance**: a misspecified model --> biased estimators  
Types of misspecification:  
  1. ommitted variable (very bad --> biased estimators)  
    Hypothesis:
    \[ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon \]
    where $y = \text{Wage}$, $x_1 = \text{Education}$, and $x_2 = \text{Ability}$
    Econometrics Model:
    \[ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \epsilon \]
    We expect: $E[\beta_i] = \beta_i$ and $E[\epsilon] = 0$
    However, ability ($x_2$) may be hard/impossible to collect data on, so we might choose to omit $\beta_2$ term. So our adjusted model is:
    \[ \tilde{y} = \tilde{\beta_0} + \tilde{\beta_1}x_1 + \mu \]
    The question we must ask is: When is $\tilde{\beta_1}$ consistent? i.e. $E[\tilde{\beta_1}] = E[\hat{\beta_1}] = \beta_1$? This is the case when 
    \[ \tilde{\beta_1} = \hat{\beta_1} + \beta_2\frac{x_2}{x_1} \equiv \hat{\beta_1} + \beta_2 \delta \]
    \[ E[\tilde{\beta_1}] = E[\hat{\beta_1} + \hat{\beta_2} \delta] = \beta_1 + \beta_2\delta \]
    So we need $\delta = \frac{x_2}{x_1} = 0$.  
  2. too many variables (not good, but not too bad --> impacts variance of model)  
  3. Functional form (incorrect f.f. --> biased estimators)  

### Ramsey RESET Test (omitted variables test)
\[ H_0: \text{All the omitted variables = 0 (i.e. we have the correct test)} \]
\[ H_1: \text{At least one omitted variable} \neq 0 \]
\[ \text{TS: } F \]
Critical region: Reject $H_0$ when $F > F_{crit}$

```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/burger78.csv')
# tr = total receipts (in thousaands of $)
# p = price (in $)
# a = advertising costs (in thousands of $)
summary(dfm)
# Theory: tr = f(p, a)
# Econometrics model: tr = b0 + b1*p + b2*a + e
# Hypothesis: b1 < 0 and b2 > 0
cor(dfm$p, dfm$a) # not too correlated, so we're okay
```
```{r}
lm.fit <- lm(tr ~ p + a, data = dfm)
summary(lm.fit)
yhat <- lm.fit$fitted.values
yhat2 <- yhat^2
yhat3 <- yhat^3
```
```{r}
# rrt = ramsey reset test
# tr.hat = b0.hat + b1.hat*p + b2.hat*a + b3.hat*yhat2 + b4.hat*yhat3 + e
lm.rrt.fit <- lm(tr ~ p + a + yhat2 + yhat3, data = dfm)
summary(lm.rrt.fit)
```
```{r}
# We want to test whether b3 = b4 = 0 for the higher-order yhat terms in the model 
anova(lm.fit)
anova(lm.rrt.fit)
anova(lm.fit, lm.rrt.fit) # anova(restricted, unrestricted)
# Since the p-value is not signficant at the 5% significance level, we are okay with omitting the higher order terms from the model.
```

### Box-Cox Test (functional form test)
\[ H_0: \text{Models are systematically similar} \]
\[ H_1: \neg H_1 \]
\[ \text{TS: } \theta = \frac{N}{2} \left| \ln{\frac{SSE_{\text{linear}}/\tilde{y_1}}{SSE_{\text{log}}}} \right| \sim \chi_1^2 \]
where $\tilde{y_1}$ is the geometric mean of the ith model.
\[ \text{Signficance: } \alpha = 0.05 \]
Critical region: Reject $H_0$ when $\theta > \chi_1$
