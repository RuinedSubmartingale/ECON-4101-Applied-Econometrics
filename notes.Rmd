---
title: "Notes on Applied Econometrics"
author: "Pranav Singh"
date: "Spring 2017"
output:
  html_document: default
  pdf_document: default
ouptut:
  pdf_document:
    toc: yes
    toc_depth: 2
---
```{r setup}
library(knitr)
# opts_knit$set(root.dir='~/Google Drive/Dalton State College/2017 Spring/ECON-4101-Applied-Econometrics/')
opts_chunk$set(cache = T)
```

# Discrete Random Variables
Let X = outcome of flipping a fair coin.
  X = 0 if heads
  X = 1 if tails
p_1 = P(X = 0) = 1/2
P_2 = P(X = 1) = 1/2

- `p_1` and `p_2` define Probability (mass/density) function

```{r}
# evansresearch.us/DSC/Spring2017/ECMT/
home.prices <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/hprice.csv', header = T)
```

# Linear regression
Economic Model  
Consumer Expenditures = f(DisposableDisposable Income)  
$y = \beta_0 + \beta_1x$  
$E[y | x] = E[\beta_0 + \beta_1x]$  
$\sigma^2 \equiv VAR(y)$  
$COV(y_i, y_j) = \emptyset \quad \forall i \neq j$  
$x_i \neq x_j$ (i.e. $x$ is not a random variable)  
$y \sim N(\beta_0 + \beta_1x, \sigma^2)$  
  
Assumption 1: $y = \underbrace{\beta_0 + \beta_1x}_{systeatic} \quad + \underbrace{e}_{unobserved}$  
Assumption 2: $E[e] = 0$  
Assumption 3: $VAR(e) = \sigma^2$  
Assumption 4: $COV(e_i, e_j) = 0$  
Assumption 5: $x_i \neq x_j$ for at least 2 values  
Assumption 6: $e \sim \mathcal{N}(0, \sigma^2)$  

We want to minimize the least squares error $LSE = (\hat{y} - \hat{r}(x))^2$, where $\hat{r}(x) = \hat{\beta_0} + \hat{\beta_1}x$. Then the estimated error is $\hat{e} = \hat{y} - \hat{r}(x)$.


```{r}
# https://stackoverflow.com/questions/7549694/ggplot2-adding-regression-line-equation-and-r2-on-graph
lm_eqn = function(m) {

  l <- list(a = format(coef(m)[1], digits = 2),
      b = format(abs(coef(m)[2]), digits = 2),
      r2 = format(summary(m)$r.squared, digits = 3));

  if (coef(m)[2] >= 0)  {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  } else {
    eq <- substitute(italic(y) == a - b %.% italic(x)*","~~italic(r)^2~"="~r2,l)    
  }

  as.character(as.expression(eq));                 
}
```

```{r}
library(ggplot2)
dispIncome.consump <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm04YC.csv', header=T)
y <- dispIncome.consump$consump; names(y) <- 'Consumption'
x <- dispIncome.consump$dispinc; names(x) <- 'Disposable Income'
boxplot(cbind(x,y), horizontal=T, col='gray')
lm.res <- lm(y ~ x)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = y ~ x) +
  annotate("text", x = min(x) + .2*(max(x) - min(x)), y = max(y), label = lm_eqn(lm.res), colour="black", size = 5, parse=TRUE)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + geom_point() + geom_smooth(method = 'loess')

cov(x,y)
cor(x,y)
res1 <- cor.test(x,y); res1
if(res1$p.value < 0.05) {
  message('significantly different from 0 at alpha=.05')
} else {
  message('not significantly different from 0 at alpha=.05')
}

# Manual calculation of estimated linear regression parameters
n <- length(x)
b1.num <- n*sum(x*y) - sum(x)*sum(y)
b1.denom <- n*sum(x^2) - sum(x)^2
b1 <- b1.num / b1.denom
b0 <- mean(y) - b1*mean(x)
round(b1,6) == round(cor(x,y)*sd(y)/sd(x), 6) # verify b1 = r * s_y / s_x
print(c(b0, b1))
print(lm.res$coefficients)
plot(x,y)
abline(b0, b1, lty=4, col='blue')
abline(h=mean(y), v=mean(x))
```

# Analysis of Variance

Total Sum of Squares = $TSS = \sum(y - \bar{y})^2$  
Sum of Squared Errors = $SSE = \sum(y - \hat{y})^2$  
Sum of Squares Regression = $SSR = \sum(\hat{y} - \bar{y})^2$  
$TSS = SSE + SSR$

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | TSS | n - 1 |

$SEE = \text{standard error of estimate} = \sqrt{MSE}$  

## Goodness of Fit Tests
$R^2 = \text{coefficient of determination} = \frac{SSR}{TSS}$  
$R^2_{Adj} = 1 - (1 + R^2)\theta$, where $\theta = \frac{n-1}{n-k-1}$ where $k$ is the number of independent variables  

### F Statistic
$H_0 : \beta_1 = 0$
$H_1 : \beta_1 \neq 0$
$\alpha = .05$
$F_{df_1, df_2} = \frac{MSR}{MSE} \text{ where } df_1 = 1 ,\; df_2 = n-2$
Reject $H_0$ if $F_{1,n-2} > F_{crit}$  
```{r}
data(mtcars)
y <- mtcars$mpg
x <- mtcars$hp
plot(x, y, pch=1, col='blue', ylab = 'mpg', xlab = 'hp')
ybar <- mean(y)
abline(h=ybar, col='blue', lwd=.5)
n <- length(y)
mod1 <- lm(y ~ x)
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
abline(coef = c(b0,b1), lty=4, col='orange')
yhat <- b0 + b1*x
points(x,yhat,pch=24,col='red')
ssr <- sum((yhat - ybar)^2)
df <- 1
msr <- ssr / df
sse <- sum( (y - yhat)^2 )
tss <- ssr + sse
r2 <- ssr / tss
round(r2, 4) == round(summary(mod1)$r.squared, 4)
k <- 1
r2adj <- 1 - (1-r2)*(n-1)/(n-k-1)
round(r2adj, 4) == round(summary(mod1)$adj.r.squared, 4)
round(tss / (n-k), 4) == round(var(y), 4)
mse <- sse / (n - k - 1)
see <- sqrt(mse)
fstat <- msr / mse
round(fstat, 4) == round(summary(mod1)$fstatistic['value'], 4)

fcrit <- qf(0.95, k, n - 2)
pf(fstat, k, n-2, lower=F)
```

# CM07
```{r}
data <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/tabb777.csv', header=T)
```
```{r}
str(data)
data$fuel <- data$fuel/1000
plot(data$hours, data$fuel)
```
```{r}
lm.mod <- lm(data = data, fuel ~ hours)
summary(lm.mod)
anova(lm.mod)
confint(lm.mod)
```
```{r}
newdata <- data.frame(hours=5:10)
cbind(newdata, predict(lm.mod, newdata, interval='confidence'))
cbind(newdata, predict(lm.mod, newdata, interval='prediction'))
```

```{r}
# download.file(url = 'http://evansresearch.us/DSC/Spring2017/ECMT/Code/ecmtUtil.R', destfile = './code/ecmUtil.R')
source('./code/ecmUtil.R')
addCIPI2Plot(data$hours, data$fuel)
```

# CMO8
### Dummy variables
Dummy variables represent qualitative (categorical) characteristics. We define
\[ 
D = 
\begin{cases}
  1 &\text{ if the characteristic is observed} \\
  0 &\text{ otherwise}
\end{cases}
\]
Dummy variable trap (including separate features for both D and ~D in the model) can lead to perfect collinearity problem.

### Functional Form
\[ \text{linear: }\; \hat{y} = a + bx + \epsilon \]
\[ \text{log-linear: }\; \ln{y} = a + bx + \epsilon \]
\[ \text{linear-log: }\; y = a + b\ln{x} + \epsilon \]
\[ \text{log-log: }\; \ln{y} = a + b\ln{x} + \epsilon \]
\[ \epsilon = \frac{ \% \Delta{Q_d} }{ \% \Delta{P} } \approx \underbrace{ \frac{ \frac{\Delta{Q_d}}{\overline{Q}}}{\frac{\Delta{P}}{\overline{P}}} }_{\text{midpoint method}} \]

```{r}
mpg <- mtcars$mpg
am <- mtcars$am # Dummy variable: am = 0 if auto, 1 if manual transmission
cyl <- mtcars$cyl
mod1 <- lm(mpg ~ am)
summary(mod1)
```
```{r}
dummy1 <- as.integer((am == 1) & (cyl == 8))
mod2 <- lm(mpg ~ dummy1)
summary(mod2)
plot(dummy1, mpg)
abline(coef(mod2))
```

### Consumption ~ Disposable.Income
\[ Real = \frac{Nominal}{CPI} * 100 \]
```{r}
dfm <- read.csv("http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm08YC.csv")
str(dfm)
y <- 100 * dfm$consump / dfm$cpi
x <- 100 * dfm$dispinc / dfm$cpi
summary(lm(y ~ x))
ly <- log(y)
lx <- log(x)
summary(lm(ly ~ lx))
```

# Multiple Regression
## Gauss-Markov Assumptions
1. Linear in parameters 
$$
\begin{aligned}
y &= \beta_0 + \sum{B_ix_i} + \epsilon \\
\ln(y) &= \beta_0 + \sum{B_ix_i} + \sum{B_j\ln(x_j)} + \epsilon \\
\end{aligned}
$$
But we can **NOT** have $y = \beta_0\cdot\beta_1 + \beta_1^2 \cdot x_1$ or the like.  
2. Random Sample $(x_i, y_i)$
3. Zero Conditional Mean \[ E(\epsilon_i|x_i) = 0 \]
4. No perfect collinearity (between any of the regressors)
    - example that fails this test: 
      \[ \text{height}_{women} = \beta_0 + \beta_1 \cdot \text{age}_{years} + \beta_2 \cdot \text{age}_{months} + \epsilon \]
5. Constant Variance \[ \mathrm{VAR}(\epsilon \vert x_i) = \sigma^2 \]
6. No Serial Correlation ("Auto-Correlation") \[ \mathrm{COV}(\epsilon_i, \epsilon_j) = 0 \]

Assumptions 5 and 6 are referred together as the **Spherical Assumptions**.

## Global F-Test
$$
\begin{aligned}
H_0 &: \beta_1 = \beta_2 = \ldots = \beta_n = 0 \\
H_1 &: \neg H_0 \\
\text{test statistic (TS): } F &= \frac{MSR}{MSE} \\
\alpha &= \mathrm{signif}(5\%) \\
\mathrm{Reject} H_0 \text{ if } F &> F_{crit, \alpha} \\
F_{crit}: & \text{ numerator df = K = # of independent vars} \\
& \text{ denominator df = N - (K + 1) = # of observations} \\
\end{aligned}
$$
### Analysis of Variance

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | SST | n - 1 |

\[ R^2 = \frac{SSR}{SST} \]
\[ S_{y|x} = \sqrt{MSE} \]

### Multi-collinearity
#### Symptoms  
* independent variables not significant yet they should be good predictors
* parameter estimates with reversed signs than what we expected
* adding/removing variables leads to substantial changes in other parameter estimates  

#### Determination
* correlation coefficient between predictors has large magnitude: $|r| > 0.7$
* **Variance Inflation Factor (VIF)**: $\mathrm{VIF} = \frac{1}{1 - R_j^2}$ where $j$ is each random variable
\[ y = \beta_0 + \sum{\beta_ix_i} \]
\[ x_1 = \alpha_0 + \sum_{i \neq 1}{\alpha_ix_i} \]
\[ R_1^2 \implies \mathrm{VIF}_1 = \frac{1}{1-R_1^2} \]
\[ R_2^2 \implies \mathrm{VIF}_2 = \frac{1}{1-R_2^2} \]

**Iterative Elimination of Offensive Variables**
1. For each independent variable, calculate `VIF`.
2. Ask the question: Are all `VIF`s < 10? If yes, then quit. Otherwise, continue.
3. Remove $\underset{x_i}{\mathrm{argmax}}(VIF_{i})$
4. Return to Step 1.

```{r}
df <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/sp500.csv')
# SP500: S&P500 index
# TB3MS: 3-month treasury bill (%)              (Hypoth: - parameter)
# NAPMNOI: new manufacturing orders (index)     (Hypoth: + parameter)
# INDPRO: industrial production (index)         (Hypoth: + parameter)
# M1: money supply                              (Hypoth: + parameter)
# HSTART: new house construction (in thousands) (Hypoth: + parameter)
# WEARN: average weekly earnings                (Hypoth: - parameter)
# PPI: producer price index (index)             (Hypoth: - parameter)
str(df)
dates <- df$YYYY.MM.DD
df$YYYY.MM.DD <- NULL
```
```{r}
mod1 <- lm(SP500 ~ ., df)
summary(mod1)
```
```{r, fig.height=6, fig.width=6}
require(corrplot)
cor1 <- cor(df)
kable(cor1)
corrplot(cor1)
```
```{r}
require(car)
crPlots(mod1)
hccm(mod1)
```
```{r}
df.x <- df[,setdiff(colnames(df), 'SP500')]
mod2 <- lm(TB3MS ~ ., df.x)
r2 <- summary(mod2)$r.squared
vif_tb <- 1 / (1-r2)
vif_tb
```
```{r}
# Using car package's vif method
require(car)
xy <- df
vif <- vif(mod1); vif
while (sort(vif, decreasing = T)[1] >= 10) {
  removeMe <- names(sort(vif, decreasing = T)[1])
  xy <- xy[,setdiff(colnames(xy), removeMe)]
  mod2 <- lm(SP500 ~ ., xy)
  vif <- vif(mod2); vif
}
vif
summary(mod2)
```
```{r}
require(lubridate)
dates <- ymd(dates)
training_idx <- which(dates < '2015-01-01')
train.xy <- xy[training_idx,]
test.xy <- xy[-training_idx,]
mod3 <- lm(SP500 ~ ., train.xy)
summary(mod3)
# Estimate of PPI changes signs and becomes statistically insignificant 
# When we built model on limited training data. So just drop it from model for now
mod3 <- lm(SP500 ~ TB3MS + NAPMNOI + INDPRO + HSTART, train.xy)
preds <- predict(mod3, test.xy)
test.results <- cbind(preds, test.xy)
test.rmse <- sqrt(mean((test.results$preds - test.results$SP500)^2))
```

# Model Specification
**Importance**: a misspecified model --> biased estimators  
Types of misspecification:  
  1. ommitted variable (very bad --> biased estimators)  
    Hypothesis:
    \[ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon \]
    where $y = \text{Wage}$, $x_1 = \text{Education}$, and $x_2 = \text{Ability}$
    Econometrics Model:
    \[ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \epsilon \]
    We expect: $E[\beta_i] = \beta_i$ and $E[\epsilon] = 0$
    However, ability ($x_2$) may be hard/impossible to collect data on, so we might choose to omit $\beta_2$ term. So our adjusted model is:
    \[ \tilde{y} = \tilde{\beta_0} + \tilde{\beta_1}x_1 + \mu \]
    The question we must ask is: When is $\tilde{\beta_1}$ consistent? i.e. $E[\tilde{\beta_1}] = E[\hat{\beta_1}] = \beta_1$? This is the case when 
    \[ \tilde{\beta_1} = \hat{\beta_1} + \beta_2\frac{x_2}{x_1} \equiv \hat{\beta_1} + \beta_2 \delta \]
    \[ E[\tilde{\beta_1}] = E[\hat{\beta_1} + \hat{\beta_2} \delta] = \beta_1 + \beta_2\delta \]
    So we need $\delta = \frac{x_2}{x_1} = 0$.  
  2. too many variables (not good, but not too bad --> impacts variance of model)  
  3. Functional form (incorrect f.f. --> biased estimators)  

### Ramsey RESET Test (omitted variables test)
\[ H_0: \text{All the omitted variables = 0 (i.e. we have the correct test)} \]
\[ H_1: \text{At least one omitted variable} \neq 0 \]
\[ \text{TS: } F \]
Critical region: Reject $H_0$ when $F > F_{crit}$


##### F Statistic
\[ F = \frac{(SSE_R - SSE_U)/q}{SSE_U/(n-k-1)}\]
\[ F \sim F_{\alpha,\; q,\; n - k - 1} \]
where $SSE_R$ is for restricted model and $SSE_U$ is for unrestricted model, and $q$ = number of restrictions, and $k$ is the number of terms in the unrestricted model.

```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/burger78.csv')
# tr = total receipts (in thousaands of $)
# p = price (in $)
# a = advertising costs (in thousands of $)
summary(dfm)
# Theory: tr = f(p, a)
# Econometrics model: tr = b0 + b1*p + b2*a + e
# Hypothesis: b1 < 0 and b2 > 0
cor(dfm$p, dfm$a) # not too correlated, so we're okay
```
```{r}
lm.restricted <- lm(tr ~ p + a, data = dfm)
summary(lm.restricted)
yhat <- lm.restricted$fitted.values
yhat2 <- yhat^2
yhat3 <- yhat^3
yhat2i <- yhat^-2
yhat3i <- yhat^-3
```
```{r}
# rrt = ramsey reset test
# tr.hat = b0.hat + b1.hat*p + b2.hat*a + b3.hat*yhat2 + b4.hat*yhat3 + e
lm.unrestricted <- lm(tr ~ p + a + yhat2 + yhat3 + yhat2i + yhat3i, data = dfm)
summary(lm.unrestricted)
```
```{r}
# We want to test whether b3 = b4 = 0 for the higher-order yhat terms in the model 
anova(lm.restricted)
anova(lm.unrestricted)
anova(lm.restricted, lm.unrestricted) # anova(restricted, unrestricted)
# Since the p-value is not signficant at the 5% significance level, we are okay with omitting the higher order terms from the model.
```
```{r}
n = nrow(dfm)
q = 4
k = 6
sse.r <- anova(lm.restricted)$`Sum Sq`[k - q + 1]; sse.r
sse.u <- anova(lm.unrestricted)$`Sum Sq`[k + 1]; sse.u
f.num <- (sse.r - sse.u)/q
f.denom <- sse.u/(n - k - 1)
f.stat <- f.num / f.denom
pf(f.stat, q, n - k - 1, lower.tail = F)
fcrit <- qf(.05, q, n - k - 1, lower.tail = F)
cat(fcrit)
cat(f.stat)
```

### Box-Cox Test (functional form test)
\[ H_0: \text{No difference in functional form (linear or log-log). i.e. Models are systematically similar} \]
\[ H_1: \neg H_1 \]
\[ \text{TS: } \theta = \frac{N}{2} \left| \ln{\frac{SSE_{\text{linear}}/\nu^2}{SSE_{\text{log}}}} \right| \sim \chi_1^2 \]
where $\nu$ is the geometric mean of the dependent variable
\[ \text{Signficance: } \alpha = 0.05 \]
Critical region: Reject $H_0$ when $\theta > \chi_1$  
If you reject, you choose the model that has $MIN \{ \frac{SSE_L}{\nu^2}, SSE_{LL}\}$

```{r}
dfm <- dfm[, c('tr', 'p', 'a')]
dfm.ll <- data.frame(sapply(dfm, log))
n <- nrow(dfm)

mod.l <- lm(tr ~ p + a, dfm)
mod.ll <- lm(tr ~ p + a, dfm.ll)

nu <- exp(mean(dfm.ll$tr)) # geometric mean
nu.sq <- nu^2; nu.sq
sse.l <- anova(mod.l)$`Sum Sq`[3]; sse.l
sse.ll <- anova(mod.ll)$`Sum Sq`[3]; sse.ll

iota <- n / 2 * abs( log( (sse.l/nu.sq)/sse.ll ) ); iota
iota.crit <- qchisq(.05, df = 1, lower.tail = F); iota.crit
iota > iota.crit
t <- sse.l / nu.sq; t

if (t < sse.ll) {
  cat('go with linear model')
} else {
  cat('go with log-log model')
}
```

# 2/27/2017
```{r}
# runs = b0 + b1*age[-] + b2*games[+] + b3*hits[+] + b4*walks[+] + b5*avg[+]
# where feature[+/-] indicates the sign we expect its coefficient to be
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/astrohitters.csv')
summary(dfm)
```
```{r}
lm.unrestricted <- lm(runs ~ Age + games + hits + walks + avg, dfm)
lm.restricted <- lm(runs ~ hits + walks, dfm)
summary(lm.unrestricted)
summary(lm.restricted)
anova(lm.restricted, lm.unrestricted) # ommitted terms appear isignificant
```

# 3/1/2017
```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/burger78.csv') # variables explained earlier above
```
Our naive model was $tr = \beta_0 + \beta_1p + \beta_2a$, but we expect the positive effect of advertising to taper off eventually. So we adjust our model to be$tr = \beta_0 + \beta_1p + \beta_2a + \beta_3a^2$, where we expect $\beta_3 < 0$.Then $\frac{dtr}{da} = \beta_2 + 2\beta_3a$. Setting this equal to 1, we get $a = \frac{1-\beta_2}{2\beta_3}$ as our estimate of when a \$1000 increase of advertising yields a \$1000 increase of total receipts. Substituting back into our model, we get $tr = \beta_0 + \beta_1p + (1-2\beta_3a)a + \beta_3a^2$, and so $tr - a = \beta_0 + \beta_1p - \beta_3a^2$. In doing so, we have imposed a restriction on our model. If your restriction doesn't truly reflect reality,
then your estimates will be biased. If they do, then it's good.

#### Cobb-Douglas Production
Q = Output, L = Labor, K = Capital
\[ Q = A L^\alpha K^\beta \]
\[ \iff \ln{Q} = \ln{A} + \alpha\ln{L} + \beta\ln{K} \equiv \beta_0 + \beta_1\ln{L} + \beta_2\ln{K} \]
\[ \text{Restriction: } \alpha + \beta = 1 \iff \beta_1 + \beta_2 = 1 \]
\[ \implies \ln{Q} = \beta_0 + (1-\beta_2)\ln{L} + \beta_2\ln{K} \]
\[ \ln{Q} - \ln{L} = \beta_0 + \beta_2(\ln{K} - \ln{Q}) \]

```{r}
# y = total receipts, x1 = price, x2 = advertising, x3 = x2^2
# yhat = b0hat + b1hat x1 + b2hat x2 + b3hat x3
dfm$asq <- dfm$a^2
mod1 <- lm(tr ~ p + a + asq, data = dfm)
summary(mod1)
```
\[ \frac{\partial \hat{y}}{\partial a} = \hat{\beta_2} + 2\hat{\beta_3}a \]
\[ 1 = \frac{\partial \hat{y}}{\partial a} \implies \frac{1-\beta_2}{2\beta_3}\]
```{r}
b2 <- mod1$coefficients[3]
b3 <- mod1$coefficients[4]
(1-b2)/(2*b3)
```
What if management said they thought $a = 40$ was optimal. How would you test this?
\[ H_0: \beta_1 + 2\beta_3(40) = 1 \]
\[ H_0: \beta_1 + 2\beta_3(40) \neq 1\]
\[ tr = \beta_0 + \beta_1p + \beta_2a + \beta_3a^2 \]
\[ tr = \beta_0 + \beta_1p + (1 - 80\beta_3)a + \beta_3a^2 \]
\[ tr - a = \beta_0 + \beta_1p + \beta_3(a^2 - 80a) \]
Let $z = tr - a$, $x_1 = p$, $x_2 = a^2 - 80a$.
Then our restricted model can be written as $z = \beta_0 + \beta_1x + \beta_3x_2$
```{r}
tr <- dfm$tr - dfm$a
p <- dfm$p
asq <- dfm$asq - 80*dfm$a
mod2 <- lm(tr ~ p + asq)
summary(mod2)
anova(mod2, mod1)
```

# 3/6/2017
```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/goodyear.csv')
# price: $1000's
# size: sqft
# bedrooms: levels
# baths: levels
str(dfm)
```
```{r}
mod1 <- lm(price ~ bedrooms + size + baths, dfm)
summary(mod1)
```
```{r}
# What's the marginal change in price for an additional 1 bedroom that is 500 sqft?
coefs <- mod1$coefficients
theta <- sum(coefs * c(0,1,500,0)) # estimate of this marginal cost
theta
```

So we've essentially enforced the following restriction onto the model: $\beta_1 + 500\beta_2 = \theta$ = `r theta`, or equivalently, $\beta_1 = \theta - 500\beta_2$. So our new model is \[ \text{price} = \beta_0 + \theta\text{bedrooms} + \beta_2(\text{size} - 500\cdot\text{bedrooms}) + \beta_3\text{baths} \]
```{r}
junk <- dfm$size - 500*dfm$bedrooms
mod2 <- lm(price ~ bedrooms + junk + baths, dfm)
summary(mod2)
mod2$coefficients['bedrooms']
```
As expected, $\theta$ = `r mod2$coefficients['bedrooms']`. We can now form a 95% confidence interval on $\theta$

```{r}
confint(mod2)
```

Now lets add some additional features to our model:
```{r}
# unrestricted model
mod1 <- lm(price ~ bedrooms + size + baths + garage + pool, dfm)
summary(mod1)

# estimate of theta:
theta2 <- sum(mod1$coefficients * c(0,1,500,0,0,0))
theta2

# restricted model
mod3 <- lm(price ~ bedrooms + junk + baths + garage + pool, dfm)
summary(mod3)
```

As expected, $\theta_2$ = `r mod3$coefficients['bedrooms']`. We can now form a 95% confidence interval on $\theta_2$
```{r}
confint(mod3)
```
We can also add the township variable to our model by first converting it into a factor variable so that the linear model uses 4 dummy variables (there are 5 unique townships, so 5 - 1 = 4).
```{r}
dfm$township <- factor(dfm$township)
mod1 <- lm(price ~ bedrooms + size + baths + garage + pool + township, dfm)
summary(mod1)
```


