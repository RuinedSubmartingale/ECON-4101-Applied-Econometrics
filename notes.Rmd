---
title: "Notes on Applied Econometrics"
author: "Pranav Singh"
date: "Spring 2017"
output:
  html_document: default
  pdf_document: default
ouptut:
  pdf_document:
    toc: yes
    toc_depth: 2
---
```{r setup}
library(knitr)
# opts_knit$set(root.dir='~/Google Drive/Dalton State College/2017 Spring/ECON-4101-Applied-Econometrics/')
opts_chunk$set(cache = T)
```

# Discrete Random Variables
Let X = outcome of flipping a fair coin.
  X = 0 if heads
  X = 1 if tails
p_1 = P(X = 0) = 1/2
P_2 = P(X = 1) = 1/2

- `p_1` and `p_2` define Probability (mass/density) function

```{r}
# evansresearch.us/DSC/Spring2017/ECMT/
home.prices <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/hprice.csv', header = T)
```

# Linear regression
Economic Model  
Consumer Expenditures = f(DisposableDisposable Income)  
$y = \beta_0 + \beta_1x$  
$E[y | x] = E[\beta_0 + \beta_1x]$  
$\sigma^2 \equiv VAR(y)$  
$COV(y_i, y_j) = \emptyset \quad \forall i \neq j$  
$x_i \neq x_j$ (i.e. $x$ is not a random variable)  
$y \sim N(\beta_0 + \beta_1x, \sigma^2)$  
  
Assumption 1: $y = \underbrace{\beta_0 + \beta_1x}_{systeatic} \quad + \underbrace{e}_{unobserved}$  
Assumption 2: $E[e] = 0$  
Assumption 3: $VAR(e) = \sigma^2$  
Assumption 4: $COV(e_i, e_j) = 0$  
Assumption 5: $x_i \neq x_j$ for at least 2 values  
Assumption 6: $e \sim \mathcal{N}(0, \sigma^2)$  

We want to minimize the least squares error $LSE = (\hat{y} - \hat{r}(x))^2$, where $\hat{r}(x) = \hat{\beta_0} + \hat{\beta_1}x$. Then the estimated error is $\hat{e} = \hat{y} - \hat{r}(x)$.


```{r}
# https://stackoverflow.com/questions/7549694/ggplot2-adding-regression-line-equation-and-r2-on-graph
lm_eqn = function(m) {

  l <- list(a = format(coef(m)[1], digits = 2),
      b = format(abs(coef(m)[2]), digits = 2),
      r2 = format(summary(m)$r.squared, digits = 3));

  if (coef(m)[2] >= 0)  {
    eq <- substitute(italic(y) == a + b %.% italic(x)*","~~italic(r)^2~"="~r2,l)
  } else {
    eq <- substitute(italic(y) == a - b %.% italic(x)*","~~italic(r)^2~"="~r2,l)    
  }

  as.character(as.expression(eq));                 
}
```

```{r}
library(ggplot2)
dispIncome.consump <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm04YC.csv', header=T)
y <- dispIncome.consump$consump; names(y) <- 'Consumption'
x <- dispIncome.consump$dispinc; names(x) <- 'Disposable Income'
boxplot(cbind(x,y), horizontal=T, col='gray')
lm.res <- lm(y ~ x)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + 
  geom_point() + 
  geom_smooth(method = 'lm', formula = y ~ x) +
  annotate("text", x = min(x) + .2*(max(x) - min(x)), y = max(y), label = lm_eqn(lm.res), colour="black", size = 5, parse=TRUE)
ggplot(dispIncome.consump, aes(x=dispinc, y=consump)) + geom_point() + geom_smooth(method = 'loess')

cov(x,y)
cor(x,y)
res1 <- cor.test(x,y); res1
if(res1$p.value < 0.05) {
  message('significantly different from 0 at alpha=.05')
} else {
  message('not significantly different from 0 at alpha=.05')
}

# Manual calculation of estimated linear regression parameters
n <- length(x)
b1.num <- n*sum(x*y) - sum(x)*sum(y)
b1.denom <- n*sum(x^2) - sum(x)^2
b1 <- b1.num / b1.denom
b0 <- mean(y) - b1*mean(x)
round(b1,6) == round(cor(x,y)*sd(y)/sd(x), 6) # verify b1 = r * s_y / s_x
print(c(b0, b1))
print(lm.res$coefficients)
plot(x,y)
abline(b0, b1, lty=4, col='blue')
abline(h=mean(y), v=mean(x))
```

# Analysis of Variance

Total Sum of Squares = $TSS = \sum(y - \bar{y})^2$  
Sum of Squared Errors = $SSE = \sum(y - \hat{y})^2$  
Sum of Squares Regression = $SSR = \sum(\hat{y} - \bar{y})^2$  
$TSS = SSE + SSR$

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | TSS | n - 1 |

$SEE = \text{standard error of estimate} = \sqrt{MSE}$  

## Goodness of Fit Tests
$R^2 = \text{coefficient of determination} = \frac{SSR}{TSS}$  
$R^2_{Adj} = 1 - (1 + R^2)\theta$, where $\theta = \frac{n-1}{n-k-1}$ where $k$ is the number of independent variables  

### F Statistic
$H_0 : \beta_1 = 0$
$H_1 : \beta_1 \neq 0$
$\alpha = .05$
$F_{df_1, df_2} = \frac{MSR}{MSE} \text{ where } df_1 = 1 ,\; df_2 = n-2$
Reject $H_0$ if $F_{1,n-2} > F_{crit}$  
```{r}
data(mtcars)
y <- mtcars$mpg
x <- mtcars$hp
plot(x, y, pch=1, col='blue', ylab = 'mpg', xlab = 'hp')
ybar <- mean(y)
abline(h=ybar, col='blue', lwd=.5)
n <- length(y)
mod1 <- lm(y ~ x)
b0 <- mod1$coefficients[1]
b1 <- mod1$coefficients[2]
abline(coef = c(b0,b1), lty=4, col='orange')
yhat <- b0 + b1*x
points(x,yhat,pch=24,col='red')
ssr <- sum((yhat - ybar)^2)
df <- 1
msr <- ssr / df
sse <- sum( (y - yhat)^2 )
tss <- ssr + sse
r2 <- ssr / tss
round(r2, 4) == round(summary(mod1)$r.squared, 4)
k <- 1
r2adj <- 1 - (1-r2)*(n-1)/(n-k-1)
round(r2adj, 4) == round(summary(mod1)$adj.r.squared, 4)
round(tss / (n-k), 4) == round(var(y), 4)
mse <- sse / (n - k - 1)
see <- sqrt(mse)
fstat <- msr / mse
round(fstat, 4) == round(summary(mod1)$fstatistic['value'], 4)

fcrit <- qf(0.95, k, n - 2)
pf(fstat, k, n-2, lower=F)
```

# CM07
```{r}
data <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/tabb777.csv', header=T)
```
```{r}
str(data)
data$fuel <- data$fuel/1000
plot(data$hours, data$fuel)
```
```{r}
lm.mod <- lm(data = data, fuel ~ hours)
summary(lm.mod)
anova(lm.mod)
confint(lm.mod)
```
```{r}
newdata <- data.frame(hours=5:10)
cbind(newdata, predict(lm.mod, newdata, interval='confidence'))
cbind(newdata, predict(lm.mod, newdata, interval='prediction'))
```

```{r}
# download.file(url = 'http://evansresearch.us/DSC/Spring2017/ECMT/Code/ecmtUtil.R', destfile = './code/ecmUtil.R')
source('./code/ecmUtil.R')
addCIPI2Plot(data$hours, data$fuel)
```

# CMO8
### Dummy variables
Dummy variables represent qualitative (categorical) characteristics. We define
\[ 
D = 
\begin{cases}
  1 &\text{ if the characteristic is observed} \\
  0 &\text{ otherwise}
\end{cases}
\]
Dummy variable trap (including separate features for both D and ~D in the model) can lead to perfect collinearity problem.

### Functional Form
\[ \text{linear: }\; \hat{y} = a + bx + \epsilon \]
\[ \text{log-linear: }\; \ln{y} = a + bx + \epsilon \]
\[ \text{linear-log: }\; y = a + b\ln{x} + \epsilon \]
\[ \text{log-log: }\; \ln{y} = a + b\ln{x} + \epsilon \]
\[ \epsilon = \frac{ \% \Delta{Q_d} }{ \% \Delta{P} } \approx \underbrace{ \frac{ \frac{\Delta{Q_d}}{\overline{Q}}}{\frac{\Delta{P}}{\overline{P}}} }_{\text{midpoint method}} \]

```{r}
mpg <- mtcars$mpg
am <- mtcars$am # Dummy variable: am = 0 if auto, 1 if manual transmission
cyl <- mtcars$cyl
mod1 <- lm(mpg ~ am)
summary(mod1)
```
```{r}
dummy1 <- as.integer((am == 1) & (cyl == 8))
mod2 <- lm(mpg ~ dummy1)
summary(mod2)
plot(dummy1, mpg)
abline(coef(mod2))
```

### Consumption ~ Disposable.Income
\[ Real = \frac{Nominal}{CPI} * 100 \]
```{r}
dfm <- read.csv("http://evansresearch.us/DSC/Spring2017/ECMT/Data/cm08YC.csv")
str(dfm)
y <- 100 * dfm$consump / dfm$cpi
x <- 100 * dfm$dispinc / dfm$cpi
summary(lm(y ~ x))
ly <- log(y)
lx <- log(x)
summary(lm(ly ~ lx))
```

# Multiple Regression
## Gauss-Markov Assumptions
1. Linear in parameters 
$$
\begin{aligned}
y &= \beta_0 + \sum{B_ix_i} + \epsilon \\
\ln(y) &= \beta_0 + \sum{B_ix_i} + \sum{B_j\ln(x_j)} + \epsilon \\
\end{aligned}
$$
But we can **NOT** have $y = \beta_0\cdot\beta_1 + \beta_1^2 \cdot x_1$ or the like.  
2. Random Sample $(x_i, y_i)$
3. Zero Conditional Mean \[ E(\epsilon_i|x_i) = 0 \]
4. No perfect collinearity (between any of the regressors)
    - example that fails this test: 
      \[ \text{height}_{women} = \beta_0 + \beta_1 \cdot \text{age}_{years} + \beta_2 \cdot \text{age}_{months} + \epsilon \]
5. Constant Variance \[ \mathrm{VAR}(\epsilon \vert x_i) = \sigma^2 \]
6. No Serial Correlation ("Auto-Correlation") \[ \mathrm{COV}(\epsilon_i, \epsilon_j) = 0 \]

Assumptions 5 and 6 are referred together as the **Spherical Assumptions**.

## Global F-Test
$$
\begin{aligned}
H_0 &: \beta_1 = \beta_2 = \ldots = \beta_n = 0 \\
H_1 &: \neg H_0 \\
\text{test statistic (TS): } F &= \frac{MSR}{MSE} \\
\alpha &= \mathrm{signif}(5\%) \\
\mathrm{Reject} H_0 \text{ if } F &> F_{crit, \alpha} \\
F_{crit}: & \text{ numerator df = K = # of independent vars} \\
& \text{ denominator df = N - (K + 1) = # of observations} \\
\end{aligned}
$$
### Analysis of Variance

|           | SS  | df    | MS = SS / df
---         | --- | ---   | ---
Regression  | SSR | k     | MSR = SSR / df
Error       | SSE | n - k - 1 | MSE = SSE / df
Total       | SST | n - 1 |

\[ R^2 = \frac{SSR}{SST} \]
\[ S_{y|x} = \sqrt{MSE} \]

### Multi-collinearity
#### Symptoms  
* independent variables not significant yet they should be good predictors
* parameter estimates with reversed signs than what we expected
* adding/removing variables leads to substantial changes in other parameter estimates  

#### Determination
* correlation coefficient between predictors has large magnitude: $|r| > 0.7$
* **Variance Inflation Factor (VIF)**: $\mathrm{VIF} = \frac{1}{1 - R_j^2}$ where $j$ is each random variable
\[ y = \beta_0 + \sum{\beta_ix_i} \]
\[ x_1 = \alpha_0 + \sum_{i \neq 1}{\alpha_ix_i} \]
\[ R_1^2 \implies \mathrm{VIF}_1 = \frac{1}{1-R_1^2} \]
\[ R_2^2 \implies \mathrm{VIF}_2 = \frac{1}{1-R_2^2} \]

**Iterative Elimination of Offensive Variables**
1. For each independent variable, calculate `VIF`.
2. Ask the question: Are all `VIF`s < 10? If yes, then quit. Otherwise, continue.
3. Remove $\underset{x_i}{\mathrm{argmax}}(VIF_{i})$
4. Return to Step 1.

```{r}
df <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/sp500.csv')
# SP500: S&P500 index
# TB3MS: 3-month treasury bill (%)              (Hypoth: - parameter)
# NAPMNOI: new manufacturing orders (index)     (Hypoth: + parameter)
# INDPRO: industrial production (index)         (Hypoth: + parameter)
# M1: money supply                              (Hypoth: + parameter)
# HSTART: new house construction (in thousands) (Hypoth: + parameter)
# WEARN: average weekly earnings                (Hypoth: - parameter)
# PPI: producer price index (index)             (Hypoth: - parameter)
str(df)
dates <- df$YYYY.MM.DD
df$YYYY.MM.DD <- NULL
```
```{r}
mod1 <- lm(SP500 ~ ., df)
summary(mod1)
```
```{r, fig.height=6, fig.width=6}
require(corrplot)
cor1 <- cor(df)
kable(cor1)
corrplot(cor1)
```
```{r}
require(car)
crPlots(mod1)
hccm(mod1)
```
```{r}
df.x <- df[,setdiff(colnames(df), 'SP500')]
mod2 <- lm(TB3MS ~ ., df.x)
r2 <- summary(mod2)$r.squared
vif_tb <- 1 / (1-r2)
vif_tb
```
```{r}
# Using car package's vif method
require(car)
xy <- df
vif <- vif(mod1); vif
while (sort(vif, decreasing = T)[1] >= 10) {
  removeMe <- names(sort(vif, decreasing = T)[1])
  xy <- xy[,setdiff(colnames(xy), removeMe)]
  mod2 <- lm(SP500 ~ ., xy)
  vif <- vif(mod2); vif
}
vif
summary(mod2)
```
```{r}
require(lubridate)
dates <- ymd(dates)
training_idx <- which(dates < '2015-01-01')
train.xy <- xy[training_idx,]
test.xy <- xy[-training_idx,]
mod3 <- lm(SP500 ~ ., train.xy)
summary(mod3)
# Estimate of PPI changes signs and becomes statistically insignificant 
# When we built model on limited training data. So just drop it from model for now
mod3 <- lm(SP500 ~ TB3MS + NAPMNOI + INDPRO + HSTART, train.xy)
preds <- predict(mod3, test.xy)
test.results <- cbind(preds, test.xy)
test.rmse <- sqrt(mean((test.results$preds - test.results$SP500)^2))
```

# Model Specification
**Importance**: a misspecified model --> biased estimators  
Types of misspecification:  
  1. ommitted variable (very bad --> biased estimators)  
    Hypothesis:
    \[ y = \beta_0 + \beta_1x_1 + \beta_2x_2 + \epsilon \]
    where $y = \text{Wage}$, $x_1 = \text{Education}$, and $x_2 = \text{Ability}$
    Econometrics Model:
    \[ \hat{y} = \hat{\beta_0} + \hat{\beta_1}x_1 + \hat{\beta_2}x_2 + \epsilon \]
    We expect: $E[\beta_i] = \beta_i$ and $E[\epsilon] = 0$
    However, ability ($x_2$) may be hard/impossible to collect data on, so we might choose to omit $\beta_2$ term. So our adjusted model is:
    \[ \tilde{y} = \tilde{\beta_0} + \tilde{\beta_1}x_1 + \mu \]
    The question we must ask is: When is $\tilde{\beta_1}$ consistent? i.e. $E[\tilde{\beta_1}] = E[\hat{\beta_1}] = \beta_1$? This is the case when 
    \[ \tilde{\beta_1} = \hat{\beta_1} + \beta_2\frac{x_2}{x_1} \equiv \hat{\beta_1} + \beta_2 \delta \]
    \[ E[\tilde{\beta_1}] = E[\hat{\beta_1} + \hat{\beta_2} \delta] = \beta_1 + \beta_2\delta \]
    So we need $\delta = \frac{x_2}{x_1} = 0$.  
  2. too many variables (not good, but not too bad --> impacts variance of model)  
  3. Functional form (incorrect f.f. --> biased estimators)  

### Ramsey RESET Test (omitted variables test)
\[ H_0: \text{All the omitted variables = 0 (i.e. we have the correct test)} \]
\[ H_1: \text{At least one omitted variable} \neq 0 \]
\[ \text{TS: } F \]
Critical region: Reject $H_0$ when $F > F_{crit}$


##### F Statistic
\[ F = \frac{(SSE_R - SSE_U)/q}{SSE_U/(n-k-1)}\]
\[ F \sim F_{\alpha,\; q,\; n - k - 1} \]
where $SSE_R$ is for restricted model and $SSE_U$ is for unrestricted model, and $q$ = number of restrictions, and $k$ is the number of terms in the unrestricted model.

```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/burger78.csv')
# tr = total receipts (in thousaands of $)
# p = price (in $)
# a = advertising costs (in thousands of $)
summary(dfm)
# Theory: tr = f(p, a)
# Econometrics model: tr = b0 + b1*p + b2*a + e
# Hypothesis: b1 < 0 and b2 > 0
cor(dfm$p, dfm$a) # not too correlated, so we're okay
```
```{r}
lm.restricted <- lm(tr ~ p + a, data = dfm)
summary(lm.restricted)
yhat <- lm.restricted$fitted.values
yhat2 <- yhat^2
yhat3 <- yhat^3
yhat2i <- yhat^-2
yhat3i <- yhat^-3
```
```{r}
# rrt = ramsey reset test
# tr.hat = b0.hat + b1.hat*p + b2.hat*a + b3.hat*yhat2 + b4.hat*yhat3 + e
lm.unrestricted <- lm(tr ~ p + a + yhat2 + yhat3 + yhat2i + yhat3i, data = dfm)
summary(lm.unrestricted)
```
```{r}
# We want to test whether b3 = b4 = 0 for the higher-order yhat terms in the model 
anova(lm.restricted)
anova(lm.unrestricted)
anova(lm.restricted, lm.unrestricted) # anova(restricted, unrestricted)
# Since the p-value is not signficant at the 5% significance level, we are okay with omitting the higher order terms from the model.
```
```{r}
n = nrow(dfm)
q = 4
k = 6
sse.r <- anova(lm.restricted)$`Sum Sq`[k - q + 1]; sse.r
sse.u <- anova(lm.unrestricted)$`Sum Sq`[k + 1]; sse.u
f.num <- (sse.r - sse.u)/q
f.denom <- sse.u/(n - k - 1)
f.stat <- f.num / f.denom
pf(f.stat, q, n - k - 1, lower.tail = F)
fcrit <- qf(.05, q, n - k - 1, lower.tail = F)
cat(fcrit)
cat(f.stat)
```

### Box-Cox Test (functional form test)
\[ H_0: \text{No difference in functional form (linear or log-log). i.e. Models are systematically similar} \]
\[ H_1: \neg H_1 \]
\[ \text{TS: } \theta = \frac{N}{2} \left| \ln{\frac{SSE_{\text{linear}}/\nu^2}{SSE_{\text{log}}}} \right| \sim \chi_1^2 \]
where $\nu$ is the geometric mean of the dependent variable
\[ \text{Signficance: } \alpha = 0.05 \]
Critical region: Reject $H_0$ when $\theta > \chi_1$  
If you reject, you choose the model that has $MIN \{ \frac{SSE_L}{\nu^2}, SSE_{LL}\}$

```{r}
dfm <- dfm[, c('tr', 'p', 'a')]
dfm.ll <- data.frame(sapply(dfm, log))
n <- nrow(dfm)

mod.l <- lm(tr ~ p + a, dfm)
mod.ll <- lm(tr ~ p + a, dfm.ll)

nu <- exp(mean(dfm.ll$tr)) # geometric mean
nu.sq <- nu^2; nu.sq
sse.l <- anova(mod.l)$`Sum Sq`[3]; sse.l
sse.ll <- anova(mod.ll)$`Sum Sq`[3]; sse.ll

iota <- n / 2 * abs( log( (sse.l/nu.sq)/sse.ll ) ); iota
iota.crit <- qchisq(.05, df = 1, lower.tail = F); iota.crit
iota > iota.crit
t <- sse.l / nu.sq; t

if (t < sse.ll) {
  cat('go with linear model')
} else {
  cat('go with log-log model')
}
```

# 2/27/2017
```{r}
# runs = b0 + b1*age[-] + b2*games[+] + b3*hits[+] + b4*walks[+] + b5*avg[+]
# where feature[+/-] indicates the sign we expect its coefficient to be
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/astrohitters.csv')
summary(dfm)
```
```{r}
lm.unrestricted <- lm(runs ~ Age + games + hits + walks + avg, dfm)
lm.restricted <- lm(runs ~ hits + walks, dfm)
summary(lm.unrestricted)
summary(lm.restricted)
anova(lm.restricted, lm.unrestricted) # ommitted terms appear isignificant
```

# 3/1/2017
```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/burger78.csv') # variables explained earlier above
```
Our naive model was $tr = \beta_0 + \beta_1p + \beta_2a$, but we expect the positive effect of advertising to taper off eventually. So we adjust our model to be$tr = \beta_0 + \beta_1p + \beta_2a + \beta_3a^2$, where we expect $\beta_3 < 0$.Then $\frac{dtr}{da} = \beta_2 + 2\beta_3a$. Setting this equal to 1, we get $a = \frac{1-\beta_2}{2\beta_3}$ as our estimate of when a \$1000 increase of advertising yields a \$1000 increase of total receipts. Substituting back into our model, we get $tr = \beta_0 + \beta_1p + (1-2\beta_3a)a + \beta_3a^2$, and so $tr - a = \beta_0 + \beta_1p - \beta_3a^2$. In doing so, we have imposed a restriction on our model. If your restriction doesn't truly reflect reality,
then your estimates will be biased. If they do, then it's good.

#### Cobb-Douglas Production
Q = Output, L = Labor, K = Capital
\[ Q = A L^\alpha K^\beta \]
\[ \iff \ln{Q} = \ln{A} + \alpha\ln{L} + \beta\ln{K} \equiv \beta_0 + \beta_1\ln{L} + \beta_2\ln{K} \]
\[ \text{Restriction: } \alpha + \beta = 1 \iff \beta_1 + \beta_2 = 1 \]
\[ \implies \ln{Q} = \beta_0 + (1-\beta_2)\ln{L} + \beta_2\ln{K} \]
\[ \ln{Q} - \ln{L} = \beta_0 + \beta_2(\ln{K} - \ln{L}) \]

```{r}
# y = total receipts, x1 = price, x2 = advertising, x3 = x2^2
# yhat = b0hat + b1hat x1 + b2hat x2 + b3hat x3
dfm$asq <- dfm$a^2
mod1 <- lm(tr ~ p + a + asq, data = dfm)
summary(mod1)
```
\[ \frac{\partial \hat{y}}{\partial a} = \hat{\beta_2} + 2\hat{\beta_3}a \]
\[ 1 = \frac{\partial \hat{y}}{\partial a} \implies \frac{1-\beta_2}{2\beta_3}\]
```{r}
b2 <- mod1$coefficients[3]
b3 <- mod1$coefficients[4]
(1-b2)/(2*b3)
```
What if management said they thought $a = 40$ was optimal. How would you test this?
\[ H_0: \beta_1 + 2\beta_3(40) = 1 \]
\[ H_0: \beta_1 + 2\beta_3(40) \neq 1\]
\[ tr = \beta_0 + \beta_1p + \beta_2a + \beta_3a^2 \]
\[ tr = \beta_0 + \beta_1p + (1 - 80\beta_3)a + \beta_3a^2 \]
\[ tr - a = \beta_0 + \beta_1p + \beta_3(a^2 - 80a) \]
Let $z = tr - a$, $x_1 = p$, $x_2 = a^2 - 80a$.
Then our restricted model can be written as $z = \beta_0 + \beta_1x + \beta_3x_2$
```{r}
tr <- dfm$tr - dfm$a
p <- dfm$p
asq <- dfm$asq - 80*dfm$a
mod2 <- lm(tr ~ p + asq)
summary(mod2)
anova(mod2, mod1)
```

# 3/6/2017
```{r}
dfm <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/goodyear.csv')
# price: $1000's
# size: sqft
# bedrooms: levels
# baths: levels
str(dfm)
```
```{r}
mod1 <- lm(price ~ bedrooms + size + baths, dfm)
summary(mod1)
```
```{r}
# What's the marginal change in price for an additional 1 bedroom that is 500 sqft?
coefs <- mod1$coefficients
theta <- sum(coefs * c(0,1,500,0)) # estimate of this marginal cost
theta
```

So we've essentially enforced the following restriction onto the model: $\beta_1 + 500\beta_2 = \theta$ = `r theta`, or equivalently, $\beta_1 = \theta - 500\beta_2$. So our new model is \[ \text{price} = \beta_0 + \theta\text{bedrooms} + \beta_2(\text{size} - 500\cdot\text{bedrooms}) + \beta_3\text{baths} \]
```{r}
junk <- dfm$size - 500*dfm$bedrooms
mod2 <- lm(price ~ bedrooms + junk + baths, dfm)
summary(mod2)
mod2$coefficients['bedrooms']
```
As expected, $\theta$ = `r mod2$coefficients['bedrooms']`. We can now form a 95% confidence interval on $\theta$

```{r}
confint(mod2)
```

Now lets add some additional features to our model:
```{r}
# unrestricted model
mod1 <- lm(price ~ bedrooms + size + baths + garage + pool, dfm)
summary(mod1)

# estimate of theta:
theta2 <- sum(mod1$coefficients * c(0,1,500,0,0,0))
theta2

# restricted model
mod3 <- lm(price ~ bedrooms + junk + baths + garage + pool, dfm)
summary(mod3)
```

As expected, $\theta_2$ = `r mod3$coefficients['bedrooms']`. We can now form a 95% confidence interval on $\theta_2$
```{r}
confint(mod3)
```
We can also add the township variable to our model by first converting it into a factor variable so that the linear model uses 4 dummy variables (there are 5 unique townships, so 5 - 1 = 4).
```{r}
dfm$township <- factor(dfm$township)
mod1 <- lm(price ~ bedrooms + size + baths + garage + pool + township, dfm)
summary(mod1)
```

# 3/8/2017
\[ y = \beta_0 + \beta_1x + \gamma_0D + \gamma_1(Dx) \]
\[ E[y] = (\beta_0 + \gamma_0) + (\beta_1 + \gamma_1)x \quad \text{when } D = 1 \]
\[ E[y] = \beta_0 + \beta_1x \quad \text{when } D = 0 \]

$\gamma_0$ represents a vertical shift of the intercept term in the model graph, and $\gamma_1$ represents a shift of the first order term slope estimate.

CM17 hwk  
inv = b0 + b1*v + b2*k  
### Chow Test
H0: models are the same, (we can pool the data)  
H1: not H0, (we need to estimate models separately)  
F statistic = [ (SSE_r - SSE_u) / q ] / [ SSE_u / n - k - 1] ~ F_alpha, q, n - k - 1 ... where q is the number of restrictions, and k is the number of independent variables in the unrestricted model.
Reject H0: if F > F_crit
```{r}
source('cm17chow.R')
```
```{r}
# CM17 hwk
# Restricted models (separately):
# GE: y_g = b0 + b1*g1 + b2*g2
# WE: y_w = c0 + c1*w1 + c2*w2

# Unrestricted model:
# D = 0 if GE, else 1 if WE
# x = UNION(GE data, WE data)
# y = b0 + b1*x1 + b2*x2 + c0*D + c1*(D*x1) + c2*(x2)
# y = { 
#     (b0 + c0) + (b1 + c1)x1 + (b2 + c2)x2     if D = 1 (unrestricted)
#     b0 + b1*x1 + b2*x2                        if D = 0 (restricted)
# }
require(data.table)
dt <- as.data.table(read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/gewe.csv'))
dt <- melt.data.table(dt, id.vars = 'X', measure=patterns('i_', 'v_', 'k_'), variable.name = 'dummy', value.name = c('investment', 'value', 'capital'))
str(dt)
```

# 3/20/2017
## Heteroskedasticity

- refers to the circumstance in which the variability of a variable is unequal across the range of values of a second variable that predicts it.
- a linear model is no longer the best linear unbiased estimator
- causes the standard errors to be incorrect (too wide) --> confidence intervals are incorrect (biased) --> we're unable to make statistical inference using standard errors

#### Identification

1. Visually (plotting `e ~ x`)
2. Formally (Goldfeld-Quandt Test)

#### Making corrections to the standard error

1. White's Approximation

```{r}
# download.file('http://evansresearch.us/DSC/Spring2017/ECMT/Code/cm18.1_hsked.R', 'cm18.1_hsked.R')
# download.file('http://evansresearch.us/DSC/Spring2017/ECMT/Data/wfe.csv', 'wfe.csv')

# y = 40.8 + 0.13x + u
# E[u] = 0
# u ~ N(0, 37.8)
source('cm18.1_hsked.R')
```

Homoskedasticity - expect $\hat{\sigma_t} = \sigma$  
Heteroskedasticity - expect $\hat{\sigma_t} = \sigma \cdot x$

#### Goldfeld-Quandt Test
\[
GQ = \frac{SE_1}{SE_2} \sim F_{\alpha, n-1, n-1} \\
H_0: \text{Homoskedastic errors} \\
H_1: \text{Heteroskedastic errors}
\]

```{r, cache = F}
midinc <- median(income)
dfm <- data.frame(income, food)
dfm <- dfm[order(income),]
dfm.upper <- dfm[dfm$income > midinc,]
dfm.lower <- dfm[dfm$income <= midinc,]
mod.upper <- lm(dfm.upper$food ~ dfm.upper$income)
var.upper <- (summary(mod.upper)$sigma)^2 # variance of upper partition
mod.lower <- lm(dfm.lower$food ~ dfm.lower$income)
var.lower <- (summary(mod.lower)$sigma)^2 # variance of lower partition
# summary(mod.upper)
# summary(mod.lower)

GQ.stat <- var.upper / var.lower; GQ.stat
df1 <- nrow(dfm.upper) - 2
df2 <- nrow(dfm.lower) - 2
fcrit <- qf(.05, df1, df2); fcrit
p <- pf(GQ.stat, df1, df2, lower.tail = F); p
```

```{r}
invisible(require(lmtest))
gqtest(food ~ income)

# woolrdige Goldfeld-Quandt Test
mod1 <- lm(food ~ income)
e <- mod1$residuals
e2 <- e^2
summary(lm(e2 ~ income))
```


#### White's Approximation for calculating robust standard errors
```{r}
invisible(require(car))
# White's Corrected Covariance Matrix
uncorrected_errors <- summary(mod1)$coefficients[,2]; uncorrected_errors
wccm <- hccm(mod1)
whites_corrected_errors <- sqrt(diag(wccm)); whites_corrected_errors
wccm.hc0 <- hccm(mod1, type='hc0')
whites_corrected_errors_hc0 <- sqrt(diag(wccm.hc0)); whites_corrected_errors_hc0

uncorrected_confint <- confint(mod1); uncorrected_confint

param_estimates <- mod1$coefficients
t.crit <- qt(.975, df=38); t.crit
corrected_confint <- (whites_corrected_errors_hc0 %*% t(t.crit*c(-1,1))) * param_estimates; corrected_confint
```

```{r}
require(sandwich)
require(lmtest)
coeftest(mod1, vcov. = vcovHC(mod1, 'HC0'))
coefci(mod1, vcov. = vcovHC(mod1, 'HC0'))
```

# 3/27/2013
y = food expenditure  
x0 = vector of 1s
x1 = income
If errors are homoskedastic, $y = \beta_0x_0 + \beta_1x_1 + e$

## Generalized Least Squares
\[ y^* = \frac{y}{\sqrt{x}} \;,\; x_0^* = \frac{x_0}{\sqrt{x}} \;,\; x_1^* = \frac{x_1}{\sqrt{x}} \;,\; e^* = \frac{e}{\sqrt{x}} \]
\[ y^* = \beta_0 x_0^* + \beta_1 x_1^* + e^* \]

By making the above correction, we ensure errors are homoskedastic. This yields valid standard errors, and consequently valid confidence intervals and valid inferences (i.e. hypothesis tests).

```{r}
dfm.wfe <- read.csv('wfe.csv')
```
```{r}
mod1 <- lm(food ~ income, dfm.wfe)
param_estimates <- mod1$coefficients; param_estimates
errors <- summary(mod1)$coefficients[,2]; errors
rse <- summary(mod1)$sigma; rse
```
```{r}
# Generalized Least Squares Weight
w <- 1 / sqrt(dfm.wfe$income)
food.w <- dfm.wfe$food * w
income.w <- dfm.wfe$income * w
const <- w
mod.w <- lm(food.w ~ 0 + const + income.w)
param_estimates.w <- mod.w$coefficients; param_estimates
errors.w <- summary(mod.w)$coefficients[,2]; errors.w
rse.w <- summary(mod.w)$sigma
```

```{r}
# 1. Estimate food ~ income for each partition: obss: 1-20, 21-40
# 2. save the errors from each regression --> e
# 3. calculate the standard deviation of the errors: --> sd(e)
# 4. transform the variables in each partitions with their 
#     respective standard deviation
# 5. OLS on the transformed variables

attach(read.csv('wfe.csv'))
# first partition...
y <- food[1:20]
x <- income[1:20]
mod1 <- lm(y ~ x)
e1.sd <- sd(mod1$residuals)
food.1 <- y / e1.sd
income.1 <- x / e1.sd
# second partition...
y <- food[21:40]
x <- income[21:40]
mod2 <- lm(y ~ x)
e2.sd <- sd(mod2$residuals)
food.2 <- y / e2.sd
income.2 <- x / e2.sd

# combine partitions...
food.12 <- c(food.1, food.2)
income.12 <- c(income.1, income.2)

# create a vector for the constant
const.12 <- rep(c(1/e1.sd, 1/e2.sd), each = 20)

# GLS via transformed variables
mod.12 <- lm(food.12 ~ 0 + const.12 + income.12)
summary(mod.12)
```
```{r}
# Easier way...
mod.12a <- lm(food ~ income, weights = const.12^2)
summary(mod.12)
```

### Feasible Generalied Least Squares (FGLS)
```{r}
# Feasible GLS (FGLS)
mod1 <- lm(food ~ income) # assume homoskedastic errors
summary(mod1)
e1 <- mod1$residuals      # save the residuals
le2 <- log(e1^2)          # log of squared errors
mod2 <- lm(le2 ~ income)  # regress le2 on all indepenent variables
ghat <- mod2$fitted.values # fittted le2 values from previous regression
hhat <- exp(ghat)         # exponentiate the above fitted values
w <- 1 / sqrt(hhat)       # weights for GLS (WLS)

mod.fgls <- lm(food ~ income, weights = w^2)
summary(mod.fgls)
```


# 4/3/2017
Typical assumptions:
\[ 
E[\epsilon_i] = 0 \\
VAR(\epsilon_i) = \sigma^2 \\
COV(\epsilon_i, \epsilon_j) = 0
\]

## Auto-Correlation
\[ \epsilon_t = \rho \epsilon_{t-1} + \mu_t \]
If $\rho \neq 0$, then our model suffers from auto-correlation. 

### Implications of auto-correlation:
\[ 
\epsilon_t = \rho \epsilon_{t-1} + \mu_t \\
E[\epsilon_t] = 0 \\
VAR[\epsilon_t] = \frac{\sigma^2}{1 - \rho^2} \\
COV[\epsilon_t, \epsilon_{t-1}] = \rho \sigma^2 \\
COR[\epsilon_t, \epsilon_{t-1}] = \rho \\
\]

- Estimators are still unbiased
- standard errors are incorrect --> confidence intervals are too wide --> invalid inference

```{r}
# price (in takas)
# area (in acres)
df <- read.csv('http://evansresearch.us/DSC/Spring2017/ECMT/Data/sugarcane.csv')
y <- df$area
x <- df$price
```

```{r}
ggplot() + geom_point(aes(x=x, y=y))
y.l <- log(y)
x.l <- log(x)
mod1 <- lm(y.l ~ x.l)
summary(mod1)
```

Let x = elasticity
```{r}
x.OLS <- summary(mod1)$coefficients[2,1]
x.OLS.se <- summary(mod1)$coefficients[2,2]
x.OLS.ci <- confint(mod1)[2,]
```

### Durbin-Watson Test
\[
H_0: \text{errors are uncorrelated} \\
H_1: \text{errors are correlated}
\]
Test Statistic:
\[ DW = \frac{\sum{(\epsilon_t - \epsilon_{t-1})^2}}{\sum{\epsilon_t^2}}\]

Test Statistic Regions:
\[ 
DW = 2 : \text{no autocorrelation} \\
DW < 2 : \text{positive autocorrelation} \\
DW > 2 : \text{negative autocorrelation}
\]

```{r}
# Durbin-Watson Test - manual way
e <- mod1$residuals
k <- 1
n <- length(e)
e.1 <- cbind(e[k:(n-k)], e[(1+k):(n-k+1)])
# autocorrelation coefficient
ac1 <- cor(e.1[,1], e.1[,2]); ac1
dw1 <- sum((e.1[,1] - e.1[,2])^2)/sum(e^2); dw1
```

```{r}
# Durbin-Watson Test - easy way
require(lmtest)
dwtest(mod1)

# require(car)
# durbinWatsonTest(mod1)
```

```{r}
require(sandwich)
coeftest(mod1)
coeftest(mod1, vcov. = vcovHAC)
coefci(mod1)
coefci(mod1, vcov. = vcovHAC)
```

## Generalized Least Squares
```{r}
require(nlme)
mod.gls <- gls(y.l ~ x.l, correlation = corARMA(p = 1, q = 0))
summary(mod.gls)
```

```{r}
plot(mod1) # Bonus: look up Cook's Distance (Residuals vs. Leverage plot)
```


# 4/10/2017
## Auto-Correlation
```{r, code=readLines('./Code/cm22_autocorr.R')}
```

## Auto-Correlation
\[
\begin{aligned}
y_{T+1} & = \beta_0 + \beta_1x_{T+1} + e_{T+1} & \text{without AR} \\
y_{T+1} & = \beta_0 + \beta_1x_{T+1} + \rho e_{T} + e_{T+1} & \text{with AR(1)} \\
\end{aligned}
\]

1. Given $x_{T+1}$, obtain $\hat{\beta}_{0_{GLS}} \;,\; \hat{\beta}_{1_{GLS}}$ (BLUE estimators)
2. $\hat{\rho} = \left( \sum_{t=2}^T \hat{e_t}\hat{e_{t-1}} \right) / \sum_{t=2}^T \hat{e}_{t-1}^2$ (from OLS)
3. $\tilde{e}_T = y_T - \hat{\beta}_{0_{GLS}}  - \hat{\beta}_{1_{GLS}}x_T $

Then our model becomes: 
\[
\begin{aligned}
\hat{y}_{T+1} & = \hat{\beta}_{0_{GLS}}  + \hat{\beta}_{1_{GLS}}x_T + \hat{\rho} \tilde{e}_{T} + \delta_{T+1} \\
\hat{y}_{T+h} & = \hat{\beta}_{0_{GLS}}  + \hat{\beta}_{1_{GLS}}x_{T+h} + \hat{\rho}^h \tilde{e}_{T} + \delta_{T+h} \\
\hat{y}_{T+h} & = \hat{\beta}_{0_{GLS}}  + \hat{\beta}_{1_{GLS}}x_{T+h} + \delta_{T+h} & \text{assuming } \lvert \hat{\rho} \rvert < 1 \implies \hat{\rho}^h \rightarrow 0  \\
\end{aligned}
\]

```{r}

```
