---
title: |
  | ECON 4101 Econometrics
  | CM07 Homework
author: "Pranav Singh"
date: "February 4, 2017"
output: pdf_document
# output: html_document
# output: github_document
---

```{r, include=F}
library(knitr)
opts_chunk$set(tidy.opts=list(width.cutoff=80),tidy=T)
options(scipen=999)

library(ggplot2)
library(gridExtra)
theme_set(theme_bw())
```

# Problem 1 - Summary Statistics
```{r}
df <- Loblolly
summary(cbind(df$height, df$age))
```

# Problem 2 - Plots
```{r, fig.height=8, fig.width=9, tidy=F}
lm.fit <- lm(formula = height ~ age, data = df)
preds <- predict.lm(object = lm.fit, newdata = NULL, interval = 'prediction')
df.preds <- cbind(df, preds)
ggplot(df.preds, aes(x = age, y = height)) + 
  geom_point() + 
  geom_smooth(method = 'lm', se = T, level = 0.95) + 
  geom_line(aes(y=lwr), color = 'red', linetype = 'dashed') +
  geom_line(aes(y=upr), color = 'red', linetype = 'dashed')
```

# Problem 3 - Regression Analysis
```{r}
summary(lm.fit)
```
From the summary of the fitted linear model, we see that the estimated linear regression equation is \[ \widehat{height} = -1.31240 + 2.59052 \cdot age \]
The 95% confidence intervals of the estimated regression coefficients are:  
```{r}
lm.coefs <- as.data.frame(summary(lm.fit)$coefficients)
t.crit <- qt(.975, 82)
b0 <- lm.coefs[1,'Estimate']
b1 <- lm.coefs[2,'Estimate']
b0.se <- lm.coefs[1, 'Std. Error']
b1.se <- lm.coefs[2, 'Std. Error']
cat('95% CI for Intercept: ', b0 + c(-1,1)*t.crit*b0.se, sep = '\n')
cat('95% CI for Age: ', b1 + c(-1,1)*t.crit*b1.se, sep = '\n')

# Equivalently, here's a one-liner
confint(lm.fit)
```
From the model summary, we also see that both the coefficient estimates are statistically significant at the 95% confidence level. The listed p-values are for the hypothesis tests that check whether $(H_0)$ the true value of the corresponding coefficient is equal to 0, versus $H_1 = \neg H_0$. Thus, the statistically significant p-values indicate that the true value for both coefficients likely differs from 0. The confidence intervals suggest that the true intercept value is likely negative, and the true slope value is likely positive. It's also important to note that the standard error (and thus the confidence interval) for the intercept term is much wider than that for the slope term.  
The value $R^2$ = `r summary(lm.fit)$r.squared` suggests the model fits the data well in the sense that the response (height) is highly correlated with the predictor (age).

# Problem 4 - Removal of the Constant Term, $\beta_0$
The value of the intercept coefficient in the above model can be interpretated as the average depth of the sampled pine trees' seeds when planted.
```{r, fig.height=8, fig.width=9, tidy=F}
lm.fit2 <- lm(formula = height ~ 0 + age, data = df)
summary(lm.fit2)
```
We see that the estimated linear regression equation without an intercept term is \[ \widehat{height} = 2.51656 \cdot age \]
The slope coefficient is statistically significant at the 95% confidence level. The small p-value provides strong evidence for the case that tree height is associated with its age. The 95% confidence interval for the estimated slope coefficient is as follows:
```{r}
confint(lm.fit2)
```

When we fit a linear regreession model, we can decompose the variance as 
$$ \text{Sum of Squares Total (SST)} = \text{Sum of Squares Error (SSE)} + \text{Sum of Squares Regression (SSR)}$$
The coefficient of determination is defined by \[ R^2 = \frac{SSR}{SST} = 1 - \frac{SSE}{SST} \]
If the linear model is fit with an intercept term, the following values are used:
$$
\begin{aligned}
SST &= \sum{(Y_i - \bar{Y})^2} \\ 
SSE &= \sum{(Y_i - \hat{Y_i})^2} \\ 
SSR &= \sum{(\hat{Y_i} - \bar{Y})^2}
\end{aligned}
$$
However, when the model is fit without an intercept term (i.e. forced to pass through the origin), the above values don't satisfy our decomposition identity that requires $SST = SSE + SSR$. Instead, we can satisfy that identity by using the following values (the 0 subscripts here indicate regression through the origin):
$$
\begin{aligned}
SST_0 &= \sum{Y_i^2} \\
SSE_0 &= \sum{(Y_i - \hat{Y_i})^2} \\
SSR_0 &= \sum{\hat{Y_i}^2}
\end{aligned}
$$
Note that the definitions of $SSR$ and $SST$ change between the two cases, while $SSE$ remains unchanged. Moreover, \[ SST = SST_0 - n\bar{Y}^2 \] In other words, the total sum of squares of a model with an intercept term is less than or equal to that of a model without an intercept. Let $\widehat{Y}$ and $\widetilde{Y}$ denote the predicted values for the models with and without an intercept, respectively. Then if and only if:
$$
\begin{aligned}
R_0^2 & > R^2 \\
\iff 1 - \frac{SSE_0}{SST_0} & > 1 - \frac{SSE}{SST} \\
\iff \frac{SSE_0}{SSE} & < \frac{SST_0}{SST} \\
\iff \frac{SSE_0}{SSE} & < \frac{SST + n\bar{Y}^2}{SST} \\
\iff \frac{SSE_0}{SSE} & < 1 + \frac{\bar{Y}^2}{\frac{1}{n}\sum{(Y_i - \bar{Y_i})^2}}
\end{aligned}
$$
The left side of this equation is greater than one since the model through the origin is nested within the model with an intercept term. The denominator of the second term on the right side is the `MSE` of an intercept-only model. So the larger the square of response mean relative to the `MSE` of an intercept-only model, the more likely that $R_0^2 > R^2$, i.e. that a regression through the origin fits the data better than a regression with an intercept term.

Returning to our results, our $R_0^2$ was greater than our $R^2$ value because, in our case, $\bar{Y}^2$ = `r mean(df$age)^2` was much greater than the intercept-only model's `MSE` = `r mean((df$age - mean(df$age))^2)`.

# Problem 5 - Predictions for New Data
```{r}
df.new <- data.frame(age = 6:15, height.pred = predict(lm.fit2, data.frame(age=6:15)))
kable(df.new, digits = 4, caption = 'Predictions using Regression Through Origin')
```

```{r, echo = F}
diagPlot<-function(model){
    p1<-ggplot(model, aes(.fitted, .resid))+geom_point()
    p1<-p1+stat_smooth(method="loess")+geom_hline(yintercept=0, col="red", linetype="dashed")
    p1<-p1+xlab("Fitted values")+ylab("Residuals")
    p1<-p1+ggtitle("Residual vs Fitted Plot")+theme_bw()
    
    p2<-ggplot(model, aes(qqnorm(.stdresid)[[1]], .stdresid))+geom_point(na.rm = TRUE)
    p2<-p2+geom_abline(aes(qqline(.stdresid)))+xlab("Theoretical Quantiles")+ylab("Standardized Residuals")
    p2<-p2+ggtitle("Normal Q-Q")+theme_bw()
    
    p3<-ggplot(model, aes(.fitted, sqrt(abs(.stdresid))))+geom_point(na.rm=TRUE)
    p3<-p3+stat_smooth(method="loess", na.rm = TRUE)+xlab("Fitted Value")
    p3<-p3+ylab(expression(sqrt("|Standardized residuals|")))
    p3<-p3+ggtitle("Scale-Location")+theme_bw()
    
    p4<-ggplot(model, aes(seq_along(.cooksd), .cooksd))+geom_bar(stat="identity", position="identity")
    p4<-p4+xlab("Obs. Number")+ylab("Cook's distance")
    p4<-p4+ggtitle("Cook's distance")+theme_bw()
    
    p5<-ggplot(model, aes(.hat, .stdresid))+geom_point(aes(size=.cooksd), na.rm=TRUE)
    p5<-p5+stat_smooth(method="loess", na.rm=TRUE)
    p5<-p5+xlab("Leverage")+ylab("Standardized Residuals")
    p5<-p5+ggtitle("Residual vs Leverage Plot")
    p5<-p5+scale_size_continuous("Cook's Distance", range=c(1,5))
    p5<-p5+theme_bw()+theme(legend.position="bottom")
    
    p6<-ggplot(model, aes(.hat, .cooksd))+geom_point(na.rm=TRUE)+stat_smooth(method="loess", na.rm=TRUE)
    p6<-p6+xlab("Leverage hii")+ylab("Cook's Distance")
    p6<-p6+ggtitle("Cook's dist vs Leverage hii/(1-hii)")
    p6<-p6+geom_abline(slope=seq(0,3,0.5), color="gray", linetype="dashed")
    p6<-p6+theme_bw()
    
    return(list(rvfPlot=p1, qqPlot=p2, sclLocPlot=p3, cdPlot=p4, rvlevPlot=p5, cvlPlot=p6))
}
```
